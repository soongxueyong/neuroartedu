## Robust End-to-End Offline Chinese Handwriting

## Text Page Spotter with Text Kernel

```
Zhihao Wang^1 , Yanwei Yu ()^1 ,^2 , Yibo Wang^1 , Haixu Long^1 , and Fazheng
Wang^1
```

(^1) School of Software Engineering,
University Of Science And Technology Of China
{cheshire, wrainbow, hxlong, wfzc}@mail.ustc.edu.cn
(^2) Suzhou Institute for Advanced Research,
University Of Science And Technology Of China
ywyu@ustc.edu.cn
Abstract.Offline Chinese handwriting text recognition is a long-standing
research topic in the field of pattern recognition. In previous studies, text
detection and recognition are separated, which leads to the fact that text
recognition is highly dependent on the detection results. In this paper,
we propose a robust end-to-end Chinese text page spotter framework.
It unifies text detection and text recognition with text kernel that inte-
grates global text feature information to optimize the recognition from
multiple scales, which reduces the dependence of detection and improves
the robustness of the system. Our method achieves state-of-the-art re-
sults on the CASIA-HWDB2.0-2.2 dataset and ICDAR-2013 competition
dataset. Without any language model, the correct rates are 99.12% and
94.27% for line-level recognition, and 99.03% and 94.20% for page-level
recognition, respectively. Code will be available at GitHub.
Keywords:Offline Chinese handwriting Text Page SpotterÂ·End-to-
EndÂ·RobustÂ·Text KernelÂ·Multiple Scales.

## 1 Introduction

```
Offline Chinese handwriting text recognition remains a challenging problem. The
main difficulties come from three aspects: the variety of characters, the diverse
writing styles, and the problem of character-touching. In recent years, methods
based on deep learning have greatly improved recognition performance. There
are two types of handwriting text recognition methods, page-level recognition
methods and line-level recognition methods. For offline Chinese handwriting text
recognition, most studies are based on line-level text recognition.
For line-level recognition, it is mainly classified into two research directions:
over-segmentation methods and segmentation-free methods. Over-segmentation
methods first over-segment the input text line image into a sequence of primitive
segments, then combine the segments to generate candidate character patterns,
forming a segmentation candidate lattice, and classify each candidate pattern to
```

# arXiv:2107.01547v1 [cs.CV] 4 Jul 2021

2 Zhihao Wang et al.

```
(a) Recognition of the distorted text image.
```

```
(b) Recognition of the dense text image.
```

Fig. 1.Results of End-to-end detection and recognition. The pink areas are the seg-
mentation results of kernel areas and the text boxes are generated by the center points
generation algorithm.

assign several candidate character classes to generate a character candidate lat-
tice. Wang Q F et al. [1] first proposed the over-segmentation method from the
Bayesian decision view and convert the classifier outputs to posterior probabil-
ities via confidence transformation. Song W et al. [2] proposed a deep network
using heterogeneous CNN to obtain hierarchical supervision information from
the segmentation candidate lattice.
However, the over-segmentation method has its inherent limitations. If the
text lines are not correctly segmented, it brings great difficulties to subsequent
recognition. The segmentation-free method based on deep learning is proposed
to solve this problem. Messina et al. [3] proposed multidimensional long-short
term memory recurrent neural networks (MDLSTM-RNN) using Connectionist
Temporal Classifier [4](CTC) as loss function for end-to-end text line recognition.
Xie et al. [5] proposed a CNN-ResLSTM model with a data preprocessing and
augmentation pipeline to rectify the text pictures to optimize recognition. Xiao
et al. [6] proposed a deep network with Pixel-Level Rectification to integrate
pixel-level rectification into CNN and RNN-based recognizers.
For page-level recognition, it can be classified into two-stage recognition
methods or end-to-end recognition methods. The two-stage recognition meth-
ods apply two models for text detection and recognition respectively. End-to-
end methods gradually compress the picture into several lines or a whole line
of feature maps for recognition. Bluche et al. [10] proposed a modification of

```
Robust Offline Chinese Handwriting Text Page Spotter 3
```

MDLSTMRNNs to recognize English handwritten paragraphs. Yousef et al. [11]
proposed the OrigamiNet which constantly compresses the width of the feature
map to 1 dimension for recognition. The two-stage methods generally detect the
text line first and then cut out the text line for recognition. Li X et al. [7] pro-
posed segmentation-based PSENet with progressive scale expansion to detect
arbitrary-shaped text lines. Liu Y et al. [8] proposed ABCNet based on third-
order Bezier curve for curved text line detection. Liao M et al. [9] proposed
Mask TextSpotter v3 with a Segmentation Proposal Network (SPN) and hard
RoI masking for robust scene text spotting.

It can be noted that the methods of page-level recognition without detecting
text lines lose the information of the text location. If the text layout is com-
plicated, it is difficult to correctly recognize text images with these methods.
The method of Liao M et al. [9] applying hard RoI masking into the RoI fea-
tures instead of transforming text lines may process very big feature maps and
loses information when resizing feature maps. Regardless of the line-level or the
two-stage recognition methods, the location information of the text line is ob-
tained first, and the text line is segmented and then recognized, which actually
separates the detection and recognition.

We think the detection and recognition should not be separated. The detec-
tion can only provide the local information of text lines, which makes it difficult
to utilize the global text image information during recognition. Whether the de-
tection box is larger or smaller than the ground truth box, it causes difficulties
for subsequent recognition. This is because the alignment of text line images is
in the original text page image, and the robustness is not enough for the recog-
nition. And the detection of text lines is so important that if the text line cannot
be detected well, the text line image cannot be accurately recognized. We believe
that the key to text recognition lies in accurately recognizing text and we just
need to know the approximate location of the text, instead of precise detection.

In this paper, we propose a robust end-to-end Chinese text page recognition
framework with text kernel segmentation and multi-scale information integration
to unify text detection and recognition. Our main contributions are as follows:

```
1) We propose a novel end-to-end text page recognition framework, which uti-
lizes global information to optimize detection and recognition.
2) We propose a method to align text lines with the text kernel, which is based
on center-lines to extract text line strips from feature maps.
3) We propose a text line recognition model with multi-scale information inte-
gration, which uses TCN and Self-attention instead of RNNs.
4) We have done a series of experiments to verify the effectiveness of our model
and compare it with other state-of-the-art methods. We achieve state-of-the-
art performance on both the CASIA-HWDB dataset and the ICDAR-
dataset. The page-level recognition performance of our method is even better
than the line-level recognition methods.
```

4 Zhihao Wang et al.

## 2 Method

The framework of our method is depicted in Figure 2. This framework consists
of three modules for text detection and recognition. The segmentation module is
used to generate the segmentation map of the kernel area of the text line and the
feature map of the text page. The connection module is introduced to extract
the text line feature map according to the segmentation map. The recognition
module is used for text line recognition which is based on DenseNet [15] with
TCN and Self-attention. The segmentation and recognition results are shown in
Figure 1.

```
Segmentation module Connect module
```

```
PANNet
```

```
Center-Line
```

```
Recognition module
DenseNet
CTCLoss LinearLayer +Self+TCN-attention
```

```
segmentation map
```

```
Feature map
```

```
TPS
```

```
TPS points
```

```
Text line Feature maps
Input image
```

```
Gt boxes
Projection
Transformation
```

```
DICELoss
```

Fig. 2.Overview of our text page recognition framework. The dashed arrow denotes
that the text line feature maps are transformed by the center lines with the kernel
areas when predicting.

2.1 Segmentation module

The segmentation module processes the input image to generate a feature map
and a segmentation map that are one-quarter the size of the original image. In
this part, We mainly utilize the network structure of PANNet [12] for its good
performance on text segmentation of arbitrary shapes. We use ResNet34 [13] as
its backbone and change the strides of the 4 feature maps generated by back-
bone to 4, 4, 8, 8 with respect to the input image. We extract larger feature
maps because we need fine-grained feature information for text recognition. We
retain the Feature Pyramid Enhancement Module(FPEM) and Feature Fusion
Module(FFM) of PANnet for extracting and fusing feature information of differ-
ent scales and the number of repetitions of the FPEM is 4. The size of the text
line kernel area we set is 0.6 of the original size, which is enough to distinguish
different text line regions.

```
Robust Offline Chinese Handwriting Text Page Spotter 5
```

2.2 Connection module

The connection module is used to extract the text line feature map accord-
ing to the segmentation map. We believe that the feature map contains high-
dimensional information than the original image, and the feature map can gather
the text information in the kernel area, which makes the extraction of the text
line feature map more robust. We transform the feature map randomly with the
kernel area as the center, such as perspective transformation, so that the text
feature information is concentrated in the kernel area. We scale all the text line
feature maps to a height of 32-pixel for subsequent recognition.

Fig. 3.The center points are generated by continuously finding the center of the current
largest inscribed circle.

We assume that the text line is a strip, and its center-line can be considered
as passing through the center of each character. Generally speaking, the length
of a text line is greater than its height, and each text line haves one center-line.
We use the inscribed circle of the text line strip to find the text center-line. We
assume that the center of the largest inscribed circle of the text line strip falls
on the text center-line.
During training, we use perspective transformation to align the text line ac-
cording to the ground truth text box. And the aligned text line feature maps are
randomly affine transformed in the direction of the interior for data enhancement
which allows the model to learn to concentrate the text feature information in
the kernel area.
In the evaluation, we align text lines based on the segmentation map. But in
this way we can only get the contour of the text line kernel region, we also need to
get the trajectory of the text line. Here we propose the Algorithm 1 to generate
the center-line based on the contour. We need to calculate the shortest distance
between each inside point and the contour boundary, so we can get the maximum

6 Zhihao Wang et al.

inscribed circle radius of each point in the contour.Distancerepresents the
calculation of the Euclidean distance between two points.AreaandPerimeter
represent the calculation of the area and perimeter of the contour.

```
Data:contour
Result:Pointscenter
Calculate the shortest distance between each inside point and the
contour boundary asdistances;
Pointscenter= [];
minR=Area(contour)/Perimeter(contour);
whileMax(distances)> minRdo
distancemax=Max(distances);
Deletedistancemaxfromdistances;
Add thepointwith itsdistancemaxtoPointscenter;
forpointiâˆˆcontourdo
ifDistance(point,pointi)< 4 âˆ—minRthen
distanceofpointi= 0
else
pass
end
end
end
Algorithm 1:Center points generation
```

This algorithm continuously finds the largest inscribed circle in the contour
and adds its center to the point set of the contour center-line and we also add
the points that extend at both ends of the center-line to the center point set.
Figure 3 shows the process of generating the center point.
We assume that the abscissa of the start point is smaller than the endpoint.
And We can reorder them by the distance between the center points, as shown
in Algorithm 2. We use thin-plate-spline(TPS) [14] interpolation to align the
images, which can match and transform the corresponding points and minimize
the bending energy generated by the deformation.
According to the points and radius of the center-line, we can generate co-
ordinate points for TPS transformation, as shown in Figure 4. Through TPS
transformation, we can transform irregular text line strips into rectangles.

2.3 Recognition module

This module is roughly equivalent to traditional text line recognition, except that
we replaced the text line image with the text line feature map. Most methods use
RNNs to construct semantic relations for time-series feature maps. We think that
RNNs have three main disadvantages: 1) it is prone to gradient disappearance;
2) the calculation speed is slow; 3) the effect of processing long text is not good.

```
Robust Offline Chinese Handwriting Text Page Spotter 7
```

```
Data:Pointscof lengthl,minR
Result:Pointsreorderc
Pointsreorderc= [];
ifLen(Pointsc)==1then
Pointsnewc=[Pointsc] ;
else
Pointsnewc=Pointsc[: 2];
foriâ† 2 toldo
leftd=Distance(pointi,Pointsnewc[0]);
rightd=Distance(pointi,Pointsnewc[âˆ’1]);
leftrightd=Distance(Pointsnewc[0],Pointsnewc[âˆ’1]);
ifrightd> leftrightdandrightd> leftdthen
InsertpointitoPointsnewcin position 0;
else
ifleftd> leftrightdandrightd< leftdthen
InsertpointitoPointsnewcin positionâˆ’1;
else
mid= arg minjâˆˆ[1,l](Distance(pointi,Pointsc[j]) +
Distance(pointi,Pointsc[jâˆ’1]));
InsertpointitoPointsnewcin positionmid;
end
end
end
end
Algorithm 2:Center points reordering
```

Fig. 4.TPS transformation on the text picture. Actually, our method performs trans-
formation operations in the feature map.

8 Zhihao Wang et al.

We designed a multi-scale feature extraction network for text line recognition
without using RNNs. And compared with CNN, TCN and Self-attention have
stronger long-distance information integration capabilities. The structure of the
recognition module is shown in Figure 5

```
DenseNet
```

```
ð‘Ã— 32 Ã— L Ã— 512 ð‘Ã—^8 Ã—ð¿ 4 Ã—^448 ð‘Ã—^4 Ã—ð¿ 8 Ã—^492 ð‘Ã—^2 Ã—ð¿ 8 Ã—^523 ð‘Ã—ð¿ 8 Ã—^1024
```

```
Self-attention
```

```
EncodingPositional
```

```
Multi
Add & NormFeed ForwardAdd & Norm-head attention
```

```
4 Ã—
```

ð‘Ã—ð¿ 8 Ã— (^1024) ð‘Ã—ð¿ 8 Ã— 1024
DenceBlock+Transition
ð‘Ã— 16 Ã—ð¿ 2 Ã— 512
Conv2d DenceBlock+Transition DenceBlock+Transition Flatten+Conv1d
TCN
OutputHiddenHidden Input
Fig. 5.Network architecture of recognition module.
For the task of text line recognition, we think there are three levels of recog-
nition. The first is to extract features of each character based on each characterâ€™s
image information. The second is to perform auxiliary feature extraction based
on several characters surrounding every single character. The third is to optimize
features based on global text information.
For the first level, we use DenseNet as the backbone to perform further feature
extraction on the feature map, gradually compress the height of the feature map
to 1, and convert it into a time-sequential feature map. The multiplicative factor
for the number of bottleneck layers is 4. There are 32 filters added to each
Dense Layer. We added the CBAM [16] layer before each Dense Block to exploit
both spatial and channel-wise attention. And the other detailed configuration of
DenseNet is presented in Table 1.
We use the Temporal Convolutional Network(TCN) [17] for the second level
of feature extraction. The dilated convolution is used on TCNs to obtain a
larger receptive field than CNN. Compared with RNNs, it performs parallel
calculations, and the calculation speed is faster, and the gradient is more stable.
A 4-layer TCN is introduced in our model and the maximum dilation is 8 which
makes the model has a receptive field of size 32.

```
Robust Offline Chinese Handwriting Text Page Spotter 9
```

```
Table 1.The structure of DenseNet.
```

```
moudle Output size Config
Convolution 1 16 Ã—W 2 3 Ã— 3 conv, stride 2 , 512 â†’ 512
```

```
Dense Block 1 16 Ã—W 2
```

### [

```
1 Ã— 1 conv
3 Ã— 3 conv
```

### ]

### Ã— 4 , 512 â†’ 640

```
Transition Layer 1 8 Ã—W 4 3 Ã— 3 conv, stride 2 Ã— 2 , 640 â†’ 448
```

```
Dense Block 2 8 Ã—W 4
```

### [

```
1 Ã— 1 conv
3 Ã— 3 conv
```

### ]

### Ã— 8 , 448 â†’ 704

```
Transition Layer 2 4 Ã—W 8 3 Ã— 3 conv, stride 2 Ã— 2 , 704 â†’ 492
```

```
Dense Block 3 4 Ã—W 8
```

### [

```
1 Ã— 1 conv
3 Ã— 3 conv
```

### ]

### Ã— 8 , 492 â†’ 748

```
Transition Layer 3 2 Ã—W 8 3 Ã— 3 conv, stride 2 Ã— 1 , 748 â†’ 523
Flatten W 8 523 â†’ 1046
Convolution 2 W 8 1 conv, 1046 â†’ 1024
```

For the third level, we adopt the structure of Self-attention for global in-
formation extraction. We only use the Encoder layer of Transformer [18], and
the multi-head self-attention mechanism is the core to build global feature con-
nections. The self-attention mechanism is mainly used in the field of natural
language processing, but the text obtained by text recognition also has semantic
features. We can use Self-attention to construct semantic association to assist
recognition. A 4-layer Self-attention encoder is applied in our model, and the
hidden layer dimension is 1024 with 16 parallel attention heads.

2.4 Loss Function

The loss function is as follows:

```
L=Ltext+Î±Lkernel (1)
```

Ltextis the character recognition loss, calculated with CTC, andLkernelis
the loss of the kernel, andÎ±is to balance the importance of the two, we setÎ±
to 0.1.
Considering the imbalance between the text area and the non-text area, we
use the dice coefficient as the method to evaluate the segmentation results.

```
Lkernel= 1âˆ’
```

### 2

### âˆ‘

```
âˆ‘ iPtext(i)Gtext(i)
iPtext(i)
```

(^2) +âˆ‘
iGtext(i)

### 2 (2)

Ptext(i) andGtext(i) represent the value of theith pixel in the segmentation
result and the ground truth of the text regions respectively. The ground truth of
the text regions is a binary image, in which the text pixel is 1 and the non-text
pixel is 0.

10 Zhihao Wang et al.

## 3 Experiment

3.1 Dataset

We use CASIA-HWDB [19] as the main dataset, which has been divided into the
train set and test set. It includes offline handwriting isolated character pictures
and offline unconstrained handwritten text page pictures. CASIA-HWDB1.0-1.
contains 3,721,874 isolated character pictures of 7,356 classes. CASIA-HWDB2.0-
2.2 contains 5,091 text page pictures, containing 1,349,414 characters of 2,
classes. CASIA-HWDB2.0-2.2 is divided into 4,076 training samples and 1,
test samples. ICDAR-2013 Chinese handwriting recognition competition Task
4 [20] is a dataset containing 300 test samples, containing 92,654 characters of
1,421 classes. The isolated character pictures of CASIA-HWDB1.0-1.2 are used
to synthesize 20,000 text page pictures, containing 9,008,396 characters. The
corpus for synthesizing is a dataset containing 2.5 million news articles.
We regenerate the text line boxes by using the smallest enclosing rectangle
method according to the outline of the text line, as shown in Fig 6. We use
the same method as PANNet to generate the text kernels that adopt the Vatti
clipping algorithm [21] to shrink the text regions by clipping d pixels. The offset
pixels d can be determined asd=A(1âˆ’r^2 )/L, whereAandLare the area
and perimeter of the polygon that represents the text region, andris the shrink
ratio, which we set to 0.6.
We convert some confusing Chinese symbols into English symbols, such as
quotation marks and commas.

```
Fig. 6.Regenerate the text boxes with the smallest enclosing rectangle.
```

3.2 Experimental Settings

We use PyTorch to implement our system. We train the whole system with the
Adam optimizer and the batch size is set to 4. We resize the pictures with a length
not greater than 1200 or a width not greater than 1600. No language model is
used to optimize the recognition results. We conducted two main experiments,
and the rest of the experimental settings are as follows:

```
Robust Offline Chinese Handwriting Text Page Spotter 11
```

```
1) Train set: train set of CASIA-HWDB2.0-2.2;
Test set: test set of CASIA-HWDB2.0-2.2;
Learning rate: initialized to 1Ã— 10 âˆ’^4 and multiplied by 0.9 every 2 epochs;
Train epoch: 50.
```

```
2) Train set: train set of CASIA-HWDB2.0-2.2 and 20,000 synthesized text
page pictures;
Test set: test set of CASIA-HWDB2.0-2.2 and test set of ICDAR-2013;
Learning rate: initialized to 5Ã— 10 âˆ’^5 and multiplied by 0.9 every 2 epochs;
Train epoch: 50;
The model weight of Experiment 1 is used for initialization, except for the
final fully connected layer.
```

```
Data:kernelboxerswith lengthm,gtboxeswith lengthl
Result:kernelboxersgroup
kernelboxersgroupis a list with lengthl;
foriâ† 0 tomdo
index= arg maxjâˆˆ[0,lâˆ’1](IOU(kernelboxers[i],gtboxes[j]);
addkernelboxers[i] tokernelboxersgroup[index]
end
Algorithm 3:Kernel boxes grouping
```

3.3 Experimental Results

With ground truth box
The ground truth boxes are used for segmentation so that our model is
equivalent to the text line recognition model. But compared with the previous
text line recognition model, our model can utilize the global information of the
text page for recognition, which has stronger anti-interference and robustness.
The experimental results are shown in Table 2. Notably, our method is superior
to the previous results by a large absolute margin of 1.22% CR and 2.14% CR
on CASIA-HWDB2.0-2.2 and ICDAR-2013 dataset without the language model.
The recognition performance of our model in CASIA-HWDB2.0-2.2 dataset is
even better than the method using the language model.
Even when the text boxes do not include the whole text line, our model can
recognize the text line correctly, as shown in Figure 7. We also conduct an exper-
iment with the size of the gt boxes randomly changed that the outline points of
gt boxes are randomly moved by -0.2 to 0.2 of the width in the vertical direction
and moved by -1 to 1 of the width in the horizontal direction, and the CR only
dropped by about 0.5%, as shown in Table 2, which shows that our model is
very robust. Because we perform segmentation and transformation of text lines
in the feature map of the text page, we expand or shrink the transformation

12 Zhihao Wang et al.

```
Fig. 7.Recognize text picture when the text box is not right.
```

box so that the text information is concentrated in the core area of the text.
Low-dimensional information is more sensitive, if such data enhancement is per-
formed directly on the original picture, some text will become incomplete and
cannot be correctly recognized.

```
Table 2.Comparison with the start-of-the-art methods.
```

```
Method
```

```
Without LM With LM
HWDB2 ICDAR-2013 HWDB2 ICDAR-
CR(%) AR(%)CR(%) AR(%)CR(%) AR(%)CR(%) AR(%)
Wu [24] - - - - 95.88 95.88 96.20 96.
Peng [23] - - 89.61 90.52 - - 94.88 94.
Song [22] 93.24 92.04 90.67 88.79 96.28 95.21 95.53 94.
Xiao [6] 97.90 97.31 - - - - - -
Xie [5] 95.37 95.37 92.13 91.55 97.28 96.97 96.99 96.
Ours, with gt box^1 99.12 98.84 - - - - - -
Ours, with center-line^1 99.03 98.64 - - - - - -
Ours, with changed gt boxc 98.58 98.22 - - - - - -
Ours, line levell 98.38 98.22 - - - - - -
Ours, with gt box^2 98.55 98.21 94.27 93.88 - - - -
Ours, with center-line^2 98.38 97.81 94.20 93.67 - - - -
```

(^1) Experiment 1. (^2) Experiment 2.
cExperiment 1 with the size of the gt boxes randomly changed.
lLine level recognition with modified recognition module.
With center-line
It can be noted that the CR calculating with center-line segmentation only
drops by about 0.1%, which fully proves the effectiveness of center-line segmen-
tation.

```
Robust Offline Chinese Handwriting Text Page Spotter 13
```

Since one line of text may be divided into multiple text lines during segmen-
tation, it is necessary to correspond the text box obtained by the segmentation
with the ground truth box. We use the method of calculating the intersection
over union(IOU) for the match, as shown in Algorithm 3. Before performing this
algorithm, it is necessary to sort all the divided boxes according to the abscissa,
so that multiple text lines are added to the corresponding group in a left-to-right
manner. Each group corresponds to one text line label. If there are extra boxes,
that is, all IOUs are calculated as 0, they can be added to any group, which does
not affect the calculation of CR and AR.
After all the segmentation boxes are divided into groups, the detection result
is considered correct when the length of the recognition result of the group is
greater than or equal to 90% of the label length. Table 3 shows the detection
performance of our model.
Effectiveness of the segmentation module
We can slightly change the structure of the recognition module to make it
a text line recognition model. The main modification is to change the 3-layer
Dense Block to the 5-layer Dense Block, from [4,8,8] to [1,4,8,8,8]. The input
data is changed to a 64-pixel text line image with a segmented height and batch
size is set to 8, and the other settings are similar to experiment 1. The recogni-
tion results show that the segmentation module can make good use of the global
text information to optimize the text feature extraction, thereby improving the
recognition performance.

```
Table 3.Text detection results on CASIA-HWDB2.0-2.2 and ICDAR-2013 dataset.
```

```
Precision(%)Recall(%)F-measure(%)
HWDB2 99.97 99.79 99.
ICDAR-2013 99.91 99.74 99.
```

Effectiveness of TCN and Self-attention
We conduct a series of ablation experiments on CASIA-HWDB2.0-2.2 dataset
without data augmentation to verify the effectiveness of TCN and Self-attention,
with results shown in Table 4. On the test set, 1.08% and 1.66% CR improve-
ments are obtained by the TCN module and the Self-attention module, respec-
tively. And CR is increased by 1.96% when the TCN module and Self-attention
module are applied together.

## 4 Conclusion

In this paper, we propose a novel robust end-to-end Chinese text page spotter
framework. It utilizes global information to concentrate the text features in the
kernel areas, which allows the model only need to roughly detect the text area

14 Zhihao Wang et al.

```
Table 4.Ablation experiment results on CASIA-HWDB2.0-2.2.
```

```
Method
with gt box with center-line
CR(%)AR(%)CR(%)AR(%)
baseline 97.18 96.81 97.09 96.
+Self-attention 98.74 98.42 98.66 98.
+TCN 98.26 97.97 98.03 97.
+TCN and Self-attention 99.12 98.84 99.03 98.
```

for recognition. TPS transformation is used to align the text lines with cen-
ter points. TCN and Self-attention are introduced into the recognition module
for multi-scale text information extraction. It can perform end-to-end text de-
tection and recognition, or optimize recognition when ground truth text boxes
are provided. This architecture can be easily modified that the segmentation
module and recognition module can be replaced by better models. Experimen-
tal results on CASIA-HWDB2.0-2.2 and ICDAR-2013 datasets show that our
method achieves state-of-the-art recognition performance. Future work will be
to investigate the performance of our method on the English dataset.

## References

1. Wang, Q., Yin, F., Liu, C. (2012). Handwritten Chinese Text Recognition by In-
   tegrating Multiple Contexts. IEEE Transactions on Pattern Analysis and Machine
   Intelligence, 34, 1469-1481.
2. Wang, S., Chen, L., Xu, L., Fan, W., Sun, J., Naoi, S. (2016). Deep Knowledge
   Training and Heterogeneous CNN for Handwritten Chinese Text Recognition. 2016
   15th International Conference on Frontiers in Handwriting Recognition (ICFHR),
   84-89.
3. Messina, R.O., Louradour, J. (2015). Segmentation-free handwritten Chinese text
   recognition with LSTM-RNN. 2015 13th International Conference on Document
   Analysis and Recognition (ICDAR), 171-175.
4. Graves, A., Fern Ìandez, S., Gomez, F., Schmidhuber, J. (2006). Connectionist tem-
   poral classification: labelling unsegmented sequence data with recurrent neural net-
   works. Proceedings of the 23rd international conference on Machine learning.
5. Xie, C., Lai, S., Liao, Q., Jin, L. (2020). High Performance Offline Handwritten Chi-
   nese Text Recognition with a New Data Preprocessing and Augmentation Pipeline.
   DAS.
6. Xiao, S., Peng, L., Yan, R., Wang, S. (2019). Deep Network with Pixel-Level Recti-
   fication and Robust Training for Handwriting Recognition. 2019 International Con-
   ference on Document Analysis and Recognition (ICDAR), 9-16.
7. Li, X., Wang, W., Hou, W., Liu, R., Lu, T., Yang, J. (2019). Shape Robust Text
   Detection With Progressive Scale Expansion Network. 2019 IEEE/CVF Conference
   on Computer Vision and Pattern Recognition (CVPR), 9328-9337.
8. Liu, Y., Chen, H., Shen, C., He, T., Jin, L., Wang, L. (2020). ABCNet: Real-Time
   Scene Text Spotting With Adaptive Bezier-Curve Network. 2020 IEEE/CVF Con-
   ference on Computer Vision and Pattern Recognition (CVPR), 9806-9815.

```
Robust Offline Chinese Handwriting Text Page Spotter 15
```

9. Liao, M., Pang, G., Huang, J., Hassner, T., Bai, X. (2020). Mask TextSpot-
   ter v3: Segmentation Proposal Network for Robust Scene Text Spotting. ArXiv,
   abs/2007.09482.
10. Bluche, T. (2016). Joint Line Segmentation and Transcription for End-to-End
    Handwritten Paragraph Recognition. NIPS.
11. Yousef, M., Bishop, T.E. (2020). OrigamiNet: Weakly-Supervised, Segmentation-
    Free, One-Step, Full Page Text Recognition by learning to unfold. 2020 IEEE/CVF
    Conference on Computer Vision and Pattern Recognition (CVPR), 14698-14707.
12. Wang, W., Xie, E., Song, X., Zang, Y., Wang, W., Lu, T., Yu, G., Shen, C. (2019).
    Efficient and Accurate Arbitrary-Shaped Text Detection With Pixel Aggregation
    Network. 2019 IEEE/CVF International Conference on Computer Vision (ICCV),
    8439-8448.
13. He, K., Zhang, X., Ren, S., Sun, J. (2016). Deep Residual Learning for Image
    Recognition. 2016 IEEE Conference on Computer Vision and Pattern Recognition
    (CVPR), 770-778.
14. Bookstein, F. (1989). Principal Warps: Thin-Plate Splines and the Decomposition
    of Deformations. IEEE Trans. Pattern Anal. Mach. Intell., 11, 567-585.
15. Huang, G., Liu, Z., Weinberger, K.Q. (2017). Densely Connected Convolutional
    Networks. 2017 IEEE Conference on Computer Vision and Pattern Recognition
    (CVPR), 2261-2269.
16. Woo, S., Park, J., Lee, J., Kweon, I. (2018). CBAM: Convolutional Block Attention
    Module. ECCV.
17. Lea, C.S., Flynn, M.D., Vidal, R., Reiter, A., Hager, G. (2017). Temporal Convo-
    lutional Networks for Action Segmentation and Detection. 2017 IEEE Conference
    on Computer Vision and Pattern Recognition (CVPR), 1003-1012.
18. Vaswani, A., Shazeer, N.M., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N.,
    Kaiser, L., Polosukhin, I. (2017). Attention is All you Need. ArXiv, abs/1706.03762.
19. Liu, C., Yin, F., Wang, D., Wang, Q. (2011). CASIA Online and Offline Chinese
    Handwriting Databases. 2011 International Conference on Document Analysis and
    Recognition, 37-41.
20. Yin, F., Wang, Q., Zhang, X., Liu, C. (2013). ICDAR 2013 Chinese Handwriting
    Recognition Competition. 2013 12th International Conference on Document Anal-
    ysis and Recognition, 1464-1470.
21. Vatti, B.R. (1992). A generic solution to polygon clipping. Commun. ACM, 35,
    56-63.
22. Wang, S., Chen, L., Xu, L., Fan, W., Sun, J., Naoi, S. (2016). Deep Knowledge
    Training and Heterogeneous CNN for Handwritten Chinese Text Recognition. 2016
    15th International Conference on Frontiers in Handwriting Recognition (ICFHR),
    84-89.
23. Peng, D., Jin, L., Wu, Y., Wang, Z., Cai, M. (2019). A Fast and Accurate Fully
    Convolutional Network for End-to-End Handwritten Chinese Text Segmentation
    and Recognition. 2019 International Conference on Document Analysis and Recog-
    nition (ICDAR), 25-30.
24. Wu, Y., Yin, F., Liu, C. (2017). Improving handwritten Chinese text recognition
    using neural network language models and convolutional neural network shape mod-
    els. Pattern Recognit., 65, 251-264.
